{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2907acbd",
   "metadata": {},
   "source": [
    "# Project Final Report\n",
    "\n",
    "### Due: Midnight on April 27 (2-hour grace period) — 50 points  \n",
    "\n",
    "### No late submissions will be accepted.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1c4bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_predict, KFold\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "combined_results = pd.DataFrame()\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.2f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n",
    "\n",
    "# Format Time for GridSearchCV\n",
    "def format_time(seconds):\n",
    "    \n",
    "    # Convert seconds to hours, minutes, and remaining seconds\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    remaining_seconds = seconds % 60\n",
    "    \n",
    "    # Return a formatted string    \n",
    "    if hours == 0 and minutes == 0:\n",
    "        return f\"{seconds:.4f}s\"\n",
    "    elif hours == 0:\n",
    "        return f\"{minutes}m {remaining_seconds:.2f}s\"\n",
    "\n",
    "    return f\"{hours}h {minutes}m {remaining_seconds:.2f}s\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa926f96",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Your final submission consists of **three components**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706edba9-eb08-4f57-bcc5-a1bb7024f068",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Final Report Notebook [40 pts]\n",
    "\n",
    "Complete all sections of this notebook to document your final decisions, results, and broader context.\n",
    "\n",
    "- **Part A**: Select the single best model from your Milestone 2 experiments. Now that you’ve finalized your model, revisit your decisions from Milestones 1 and 2. Are there any steps you would change—such as cleaning, feature engineering, or model evaluation—given what you now know?\n",
    "\n",
    "- **Part B**: Write a technical report following standard conventions, for example:\n",
    "  - [CMU guide to structure](https://www.stat.cmu.edu/~brian/701/notes/paper-structure.pdf)\n",
    "  - [Data science report example](https://www.projectpro.io/article/data-science-project-report/620)\n",
    "  - The Checklist given in this week's Blackboard Lesson (essentially the same as in HOML).\n",
    "    \n",
    "  Your audience here is technically literate but unfamiliar with your work—like your manager or other data scientists. Be clear, precise, and include both code (for illustration), charts/plots/illustrations, and explanation of what you discovered and your reasoning process. \n",
    "\n",
    "The idea here is that Part A would be a repository of the most important code, for further work to come, and Part B is\n",
    "the technical report which summarizes your project for the data science group at your company. Do NOT assume that readers of Part B are intimately familiar with Part A; provide code for illustration as needed, but not to run.\n",
    "\n",
    "Submit this notebook as a group via your team leader’s Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. PowerPoint Presentation [10 pts]\n",
    "\n",
    "Create a 10–15 minute presentation designed for a general audience (e.g., sales or marketing team).\n",
    "\n",
    "- Prepare 8–12 slides, following the general outline of the sections of Part B. \n",
    "- Focus on storytelling, visuals (plots and illustrations), and clear, simplified language. No code!\n",
    "- Use any presentation tool you like, but upload a PDF version.\n",
    "- List all team members on the first slide.\n",
    "\n",
    "Submit as a group via your team leader’s Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Individual Assessment\n",
    "\n",
    "Each team member must complete the Individual Assessment Form (same as in Milestone 1), sign it, and upload it via their own Gradescope account.\n",
    "\n",
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "-  Final Report Notebook — Team leader submission\n",
    "-  PDF Slides — Team leader submission\n",
    "-  Individual Assessment Form — Each member submits their own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60953e",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db81c2",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee15ea-7cf4-4d57-b83a-18f186f0d204",
   "metadata": {},
   "source": [
    "## Part A: Final Model and Design Reassessment [10 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model and revisit earlier decisions to determine if any should be revised in light of your complete modeling workflow. You’ll also consolidate and present the key code used to run your model on the preprocessed dataset, with thoughtful documentation of your reasoning.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "- Reconsider **at least one decision from Milestone 1** (e.g., preprocessing, feature engineering, or encoding). Explain whether you would keep or revise that decision now that you know which model performs best. Justify your reasoning.\n",
    "  \n",
    "- Reconsider **at least one decision from Milestone 2** (e.g., model evaluation, cross-validation strategy, or feature selection). Again, explain whether you would keep or revise your original decision, and why.\n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset. This section should be a clean, readable summary of the most important steps from Milestones 1 and 2, adapted as needed to fit your final model choice and your reconsiderations as just described. \n",
    "\n",
    "- Use Markdown cells and inline comments to explain the structure of the code clearly but concisely. The goal is to make your reasoning and process easy to follow for instructors and reviewers.\n",
    "\n",
    "> Remember: You are not required to change your earlier choices, but you *are* required to reflect on them and justify your final decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbcc093",
   "metadata": {},
   "source": [
    "### **Import Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52be0244-e449-4322-a036-d188724901aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.read_csv('Datasets_Results/baseline_df.csv')\n",
    "milestone_1_df = pd.read_csv('Datasets_Results/milestone_1_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e802a",
   "metadata": {},
   "source": [
    "### **Prelude: Functions Definitions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4b744",
   "metadata": {},
   "source": [
    "#### **Profile Dataset**\n",
    "\n",
    "This function was used to quickly profile the number of null and unique values within a dataset throughout our Milestone 1 and Milestone 2 notebooks. We’ve imported it here to verify that the relevant datasets were saved correctly, and because we specifically reference its results in our subsequent report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea6e52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================================================\n",
    "# This is meant to consolidate the 'show_null_counts_features' function from before with \n",
    "# another with 'value' and 'unique' counts later on in this analysis. \n",
    "# ========================================================================================\n",
    "\n",
    "def profile_dataset(df):\n",
    "    # Identify feature types\n",
    "    feature_types = df.dtypes.apply(lambda x: 'Numeric' if np.issubdtype(x, np.number) else 'Categorical')\n",
    "\n",
    "    # Build a summary DataFrame\n",
    "    summary = pd.DataFrame({\n",
    "        'Feature': df.columns,\n",
    "        'Type': feature_types.values,\n",
    "        'Null Values': df.isnull().sum().values,\n",
    "        'Null %': (df.isnull().mean() * 100).round(2).values,\n",
    "        'Count (Non-Null)': df.count().values,\n",
    "        'Unique Values': df.nunique().values\n",
    "    })\n",
    "\n",
    "    # Sort Values in Summary by % of null values\n",
    "    summary = summary.sort_values(by='Null %', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Add dataset shape info above the table\n",
    "    print(f\"This dataset contain {df.shape[0]} rows\")\n",
    "    print(f\"This dataset contain {df.shape[1]} columns\")\n",
    "\n",
    "    # Display the summary\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd19f7",
   "metadata": {},
   "source": [
    "#### **Train Test Split**\n",
    "\n",
    "Function to train test split the data provided a dataframe and a target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f96424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_data(df, target_col):\n",
    "    X = df.drop(columns=target_col)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0334243",
   "metadata": {},
   "source": [
    "#### **Standardization Function**\n",
    "\n",
    "This function applies the StandardScaler from scikit-learn to numerical features, excluding categorical columns and those that have been one-hot encoded (i.e., columns with two or fewer unique values). It is called within functions like run_model() to ensure that datasets are properly scaled when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ec2e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_features_train_and_test(X_train, X_test=None, target_column=None, debug=False, return_scaler=False):\n",
    "    \"\"\"\n",
    "    Standardize numerical features in either a single DataFrame or in X_train/X_test split.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame to scale (or the only one, for legacy use)\n",
    "    - X_test: Optional second DataFrame (for train/test split)\n",
    "    - target_column: Column to exclude from scaling\n",
    "    - debug: Print details\n",
    "    - return_scaler: Whether to return the fitted scaler\n",
    "\n",
    "    Returns:\n",
    "    - Scaled X_train (or single df), optional X_test, optional scaler\n",
    "    \"\"\"\n",
    "    legacy_mode = X_test is None  # True if using legacy single-DF mode\n",
    "\n",
    "    # In legacy mode, treat input as single DataFrame\n",
    "    if legacy_mode:\n",
    "        df_scaled = X_train.copy()\n",
    "        numeric_cols = df_scaled.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "        if target_column in numeric_cols:\n",
    "            numeric_cols.remove(target_column)\n",
    "\n",
    "        numeric_cols_to_scale = [col for col in numeric_cols if df_scaled[col].nunique(dropna=True) > 2]\n",
    "\n",
    "        if debug:\n",
    "            print(\"Scaling columns:\", numeric_cols_to_scale)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled[numeric_cols_to_scale] = scaler.fit_transform(df_scaled[numeric_cols_to_scale])\n",
    "\n",
    "        if return_scaler:\n",
    "            return df_scaled, None, scaler\n",
    "        return df_scaled  # legacy-compatible\n",
    "\n",
    "    # --- New version for X_train/X_test split ---\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy() if X_test is not None else None\n",
    "\n",
    "    numeric_cols = X_train.select_dtypes(include='number').columns.tolist()\n",
    "    if target_column and target_column in numeric_cols:\n",
    "        numeric_cols.remove(target_column)\n",
    "\n",
    "    numeric_cols_to_scale = [col for col in numeric_cols if X_train[col].nunique(dropna=True) > 2]\n",
    "\n",
    "    if debug:\n",
    "        print(\"Numeric columns to scale:\", numeric_cols_to_scale)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled[numeric_cols_to_scale] = scaler.fit_transform(X_train[numeric_cols_to_scale])\n",
    "\n",
    "    if X_test_scaled is not None:\n",
    "        X_test_scaled = X_test_scaled[X_train_scaled.columns]\n",
    "        X_test_scaled[numeric_cols_to_scale] = scaler.transform(X_test_scaled[numeric_cols_to_scale])\n",
    "\n",
    "    if return_scaler and X_test is not None: \n",
    "        return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "    if X_test is not None: \n",
    "        return X_train_scaled, X_test_scaled, None\n",
    "\n",
    "    # Always return 3 values (legacy call with just one dataset)\n",
    "    if return_scaler:\n",
    "        return X_train_scaled, None, scaler\n",
    "\n",
    "    return X_train_scaled, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab778ab0",
   "metadata": {},
   "source": [
    "#### **Run Model Function**\n",
    "\n",
    "This function served as a basis for running the models in each section of our report. It was modified from its original version in Module 3 materials and edited to include the following: \n",
    "\n",
    "- **concat_results:** A boolean parameter meant to append results to our global combined_results dataframe. We subsequently used this dataframe to store the results of each of the models throughout the course of the excersice. \n",
    "\n",
    "- **plot_pred:** A boolean parameter meant to plot the predictions of the trained model against their actual values in the training (or optional: Test set). This is meant to evaluate how the model is performing. \n",
    "\n",
    "- **target_is_logged:** Another boolean parameter meant to change the Cross Validation strategy in case the target variable was logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d483e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version with test data and scaling:\n",
    "def run_model(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test=None,\n",
    "    y_test=None,\n",
    "    n_repeats=5,\n",
    "    n_jobs=-1,\n",
    "    concat_results=False,\n",
    "    run_comment=None,\n",
    "    target_is_logged=False,\n",
    "    plot_pred=False,\n",
    "    return_scaler=False,\n",
    "    **model_params\n",
    "):\n",
    "    global combined_results\n",
    "\n",
    "    # Scale both training and test data\n",
    "    X_train_scaled, X_test_scaled, scaler = standardize_features_train_and_test(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        target_column=None,\n",
    "        return_scaler=True\n",
    "    )\n",
    "\n",
    "    model_instance = model(**model_params) if isinstance(model, type) else model\n",
    "    model_name = model.__name__ if isinstance(model, type) else model.__class__.__name__\n",
    "\n",
    "    # --- Cross-validation & training ---\n",
    "    if target_is_logged:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        rmse_scores = []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(X_train_scaled):\n",
    "            X_tr = X_train_scaled.iloc[train_idx]\n",
    "            X_val = X_train_scaled.iloc[val_idx]\n",
    "            y_tr = y_train.iloc[train_idx]\n",
    "            y_val = y_train.iloc[val_idx]\n",
    "\n",
    "            model_instance.fit(X_tr, y_tr)\n",
    "            preds_log = model_instance.predict(X_val)\n",
    "\n",
    "            y_preds = np.expm1(preds_log)\n",
    "            y_true = np.expm1(y_val)\n",
    "\n",
    "            rmse_scores.append(root_mean_squared_error(y_true, y_preds))\n",
    "\n",
    "        mean_CV_RMSE = np.mean(rmse_scores)\n",
    "        std_CV_RMSE = np.std(rmse_scores)\n",
    "\n",
    "        # Fit final model on full training set\n",
    "        model_instance.fit(X_train_scaled, y_train)\n",
    "        y_train_pred = np.expm1(model_instance.predict(X_train_scaled))\n",
    "        y_train_true = np.expm1(y_train)\n",
    "        training_RMSE = root_mean_squared_error(y_train_true, y_train_pred)\n",
    "\n",
    "        # Evaluate test set\n",
    "        test_RMSE = None\n",
    "        if X_test_scaled is not None and y_test is not None:\n",
    "            y_test_pred = np.expm1(model_instance.predict(X_test_scaled))\n",
    "            y_test_true = np.expm1(y_test)\n",
    "            test_RMSE = root_mean_squared_error(y_test_true, y_test_pred)\n",
    "\n",
    "        # Ensure test prediction is safe for plotting\n",
    "        if X_test_scaled is None or y_test is None:\n",
    "            y_test_pred = None\n",
    "\n",
    "        if plot_pred:\n",
    "            plot_predictions(\n",
    "                y_train_true=y_train_true,\n",
    "                y_train_pred=y_train_pred,\n",
    "                y_test_true=y_test_true,\n",
    "                y_test_pred=y_test_pred,\n",
    "                title=model_name + \" Train vs Test\"\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        # Not log-transformed\n",
    "        neg_mse_scores = cross_val_score(\n",
    "            model_instance,\n",
    "            X_train_scaled,\n",
    "            y_train,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=RepeatedKFold(n_splits=5, n_repeats=n_repeats, random_state=42),\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "        mean_CV_RMSE = np.mean(rmse_scores)\n",
    "        std_CV_RMSE = np.std(rmse_scores)\n",
    "\n",
    "        model_instance.fit(X_train_scaled, y_train)\n",
    "        y_train_pred = model_instance.predict(X_train_scaled)\n",
    "        training_RMSE = root_mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "        test_RMSE = None\n",
    "        if X_test_scaled is not None and y_test is not None:\n",
    "            y_test_pred = model_instance.predict(X_test_scaled)\n",
    "            test_RMSE = root_mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "        # Ensure test prediction is safe for plotting\n",
    "        if X_test_scaled is None or y_test is None:\n",
    "            y_test_pred = None\n",
    "            \n",
    "        if plot_pred:\n",
    "            plot_predictions(\n",
    "                y_train_true=y_train,\n",
    "                y_train_pred=y_train_pred,\n",
    "                y_test_true=y_test,\n",
    "                y_test_pred=y_test_pred,\n",
    "                title=model_name + \" Train vs Test\"\n",
    "            )\n",
    "\n",
    "    # --- Collect results ---\n",
    "    results_df = pd.DataFrame([{\n",
    "        \"model\": model_name,\n",
    "        \"model_params\": model_instance.get_params(),\n",
    "        \"mean_CV_RMSE\": mean_CV_RMSE,\n",
    "        \"std_CV_RMSE\": std_CV_RMSE,\n",
    "        \"training_RMSE\": training_RMSE,\n",
    "        \"test_RMSE\": test_RMSE,\n",
    "        \"run_comment\": run_comment\n",
    "    }])\n",
    "\n",
    "    if concat_results:\n",
    "        try:\n",
    "            combined_results = pd.concat([combined_results, results_df], ignore_index=True)\n",
    "        except NameError:\n",
    "            combined_results = results_df\n",
    "\n",
    "    return (results_df, scaler) if return_scaler else results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f3e80",
   "metadata": {},
   "source": [
    "#### **Run Model Suite**\n",
    "\n",
    "Throughout the project, the run_model function was used at various stages—such as baseline modeling, after feature engineering, and more—to evaluate different models. This Run Model Suite function serves as a wrapper for run_model, allowing it to be applied efficiently across a list of models when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d0c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_suite(\n",
    "    models,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test=None,\n",
    "    y_test=None,    \n",
    "    n_repeats=5,\n",
    "    n_jobs=-1,\n",
    "    concat_results=False,\n",
    "    run_comment=None,\n",
    "    target_is_logged=False,\n",
    "    plot_pred=False,\n",
    "    **model_params\n",
    "):\n",
    "    global combined_results # should not be here, ok if it works\n",
    "    total_start = time.time()\n",
    "    \n",
    "    all_results = []\n",
    "    total_models = len(models)\n",
    "    for  i, model in enumerate(models, 1):\n",
    "        print(f\"\\n[{i}/{total_models}] Running model: {model}\")\n",
    "        model_start = time.time()\n",
    "        \n",
    "        result_df = run_model(\n",
    "            model=model,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test, \n",
    "            n_repeats=n_repeats,\n",
    "            n_jobs=n_jobs,\n",
    "            run_comment=run_comment,\n",
    "            target_is_logged=target_is_logged,\n",
    "            plot_pred=plot_pred,\n",
    "            **model_params\n",
    "        )\n",
    "        model_end = time.time()\n",
    "        model_duration = model_end - model_start\n",
    "        print(f\"[{i}/{total_models}] Model completed in {model_duration:.2f} seconds.\")\n",
    "        \n",
    "        all_results.append(result_df)\n",
    "\n",
    "    combined = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    total_end = time.time()\n",
    "    total_duration = total_end - total_start\n",
    "    print(f\"\\nAll models finished. Total time: {total_duration:.2f} seconds.\")\n",
    "    \n",
    "    if concat_results:\n",
    "        try:\n",
    "            combined_results = pd.concat([combined_results, combined], ignore_index=True)\n",
    "        except NameError:\n",
    "            combined_results = combined  # first time\n",
    "       \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca41784",
   "metadata": {},
   "source": [
    "#### **Plot RMSE Comparison**\n",
    "\n",
    "This was used throughout the second milestone to plot the Training RMSE, Cross Validation RMSE (and optionally the test RMSE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e22c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rmse_comparison(df, run_comment_keyword=\"Base Model Parameters\", title=\"Training vs CV RMSE (Stacked)\", figsize=(12, 6)): \n",
    "    # Filter and sort by mean CV RMSE\n",
    "    filtered_df = df[df['run_comment'].str.contains(run_comment_keyword, case=False, na=False)]\n",
    "    filtered_df = filtered_df.sort_values(by='mean_CV_RMSE', ascending=True)\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"No data found with run_comment containing '{run_comment_keyword}'\")\n",
    "        return\n",
    "\n",
    "    # Determine which RMSE columns exist\n",
    "    rmse_columns = ['training_RMSE', 'mean_CV_RMSE']\n",
    "    if 'test_RMSE' in filtered_df.columns and filtered_df['test_RMSE'].notna().any():\n",
    "        rmse_columns.append('test_RMSE')\n",
    "\n",
    "    # Melt into long form for seaborn\n",
    "    stacked_df = filtered_df[['model'] + rmse_columns].melt(\n",
    "        id_vars='model',\n",
    "        var_name='RMSE_Type',\n",
    "        value_name='RMSE_Value'\n",
    "    )\n",
    "\n",
    "    # Plot setup\n",
    "    custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "    sns.set_theme(style=\"ticks\", rc=custom_params)\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Bar plot\n",
    "    barplot = sns.barplot(\n",
    "        data=stacked_df,\n",
    "        x='model',\n",
    "        y='RMSE_Value',\n",
    "        hue='RMSE_Type'\n",
    "    )\n",
    "\n",
    "    # Add labels to bars\n",
    "    for bar in barplot.patches:\n",
    "        height = bar.get_height()\n",
    "        if not pd.isna(height):\n",
    "            x = bar.get_x() + bar.get_width() / 2\n",
    "            label = f\"${height:,.0f}\"\n",
    "            barplot.text(x, height + 3000, label, ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # Format y-axis\n",
    "    barplot.yaxis.set_major_formatter(mticker.FuncFormatter(dollar_format))\n",
    "\n",
    "    # Final layout\n",
    "    plt.title(f\"{title}: {run_comment_keyword}\", fontsize=14, fontweight='bold')\n",
    "    plt.ylabel(\"RMSE\", fontsize=12)\n",
    "    plt.xlabel(\"Model\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653ea3a2-adf1-4e1f-bc9c-161e292bfafb",
   "metadata": {},
   "source": [
    "**And don't forget about commentary cells!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75636fbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdf6342d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a310ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b01586a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "683e6eb7",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89dda7e",
   "metadata": {},
   "source": [
    "![Test_Figure_1](./Figures/Test_Figure_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff9376",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce548ba-c1ce-4a66-8885-9f7e40b1c404",
   "metadata": {},
   "source": [
    "## Part B: Final Data Science Project Report Assignment [30 pts]\n",
    "\n",
    "This final report is the culmination of your semester-long Data Science project, building upon the exploratory analyses and modeling milestones you've already completed. Your report should clearly communicate your findings, analysis approach, and conclusions to a technical audience. The following structure and guidelines, informed by best practices, will help you prepare a professional and comprehensive document.\n",
    "\n",
    "### Required Sections\n",
    "\n",
    "Your report must include the following sections:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7347f37a",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d6069-cf27-4312-8e4b-8c27fa6cf9d0",
   "metadata": {},
   "source": [
    "#### **1. Executive Summary (Abstract) [2 pts]**\n",
    "- **Brief overview of the entire project (150–200 words)**\n",
    "- **Clearly state the objective, approach, and key findings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d6b9a",
   "metadata": {},
   "source": [
    "TODO: I wonder if the key findings should be *after* our changes here? aka re-do the last paragraph when done? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c0517",
   "metadata": {},
   "source": [
    "**Rough Draft w/ TaxValue** The goal of this project is to develop a predictive model using Zillow’s housing dataset to estimate the tax-assessed value of residential properties (taxvaluedollarcnt). By leveraging various home features—such as square footage, location, and year built—the model aims to build on the success of Zillow’s widely-used Zestimate tool by expanding its capabilities to include predictions of a home’s tax-assessed value. This would offer homeowners greater transparency into how their property taxes—one of the most significant ongoing expenses after purchasing a home—are determined.\n",
    "\n",
    "\n",
    "Our approach is to apply regression techniques to identify key factors influencing property valuations, build a model to predict the taxvaluedollarcnt of different properties based on the data provided by Zillow, and to minimize prediction errors, as evaluated by the use of metrics like Root Mean Squared Error (RMSE). \n",
    "\n",
    "Our key findings were that while ensemble methods like Gradient Boosted Trees yielded RMSEs that were improvements over baseline estimates, models still contained an average error rate north of \\$300,000. Due to the trend of models performing significantly worse on homes with taxvalluedollarcnt over \\$2,000,000, (consistently undershooting targets) we suspect that this 'average' is doing significantly worse due to the presence of extreme outliers in the data (some close to \\$49,000,000). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a131f26",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd854f-857f-41a2-983a-845c99a87153",
   "metadata": {},
   "source": [
    "#### **2. Introduction [2 pts]**\n",
    "- **Clearly introduce the topic and context of your project**\n",
    "- **Describe the problem you are addressing (the problem statement)**\n",
    "- **Clearly state the objectives and goals of your analysis**\n",
    "\n",
    "Note: You may imaginatively consider this project as taking place in a real estate company with a small data science group in-house, and write your introduction from this point of view (don't worry about verisimilitude to an actual company!).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833864c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfcf823c-453c-40ba-89f0-07826b9adf7a",
   "metadata": {},
   "source": [
    "TODO: There was a bunch of talk in the live session about it being the taxvalue and not the market value, and that we should humor them and preted that we're pitching this in some way -- so let me know what you think\n",
    "\n",
    "**Rough Draft** \n",
    "\n",
    "Released 15 years ago, Zillow’s **Zestimate** feature revolutionized transparency in the real estate sector by giving home buyers instant access to accurate, market-based property valuations -- allowing consumers to  easily compare listing prices to market estimates with just a click. \n",
    "\n",
    "But while this market value is critical during the initial real estate transaction, there is another metric that impacts the finances of homeowners long after the initial purchase, and that is the **tax assessed value**. \n",
    "\n",
    "Yet despite property taxes represent one of the largest ongoing costs for homeowners across the nation, many homeowners have little visibility into how this value is determined, or whether their value is fair when compared to similar properties. \n",
    "\n",
    "As part of our in-house data science initiative, we aim to address this gap by leveraging machine learning to develop a model that accurately predicts a property's tax assessed value. In this, our main objectives were clear: \n",
    "\n",
    "- Identify the most important features when assessing a home's tax value. \n",
    "\n",
    "- Develop a machine learning model to accurately predict a home's tax assessed value from zillow's database. \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1eed7",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6394185-1992-469c-b475-05bd473327b1",
   "metadata": {},
   "source": [
    "#### **3. Data Description [2 pts]**\n",
    "- **Describe the source of your dataset (described in Milestone 1)**\n",
    "- **Clearly state the characteristics of your data (size, types of features, missing values, target, etc.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d762ca9d",
   "metadata": {},
   "source": [
    "**Data Set Source**\n",
    "\n",
    "\n",
    "The dataset for this project is taken from Zillow's 'Zestimate' refining Kaggle competition dataset that was run in 2017 (reference 1). The dataset was orgininally edited down to 55 columns and 77,613 rows. Although the original dataset was structured for the home value to be the target variable, the target variable in our case was 'taxvaluedollarcnt'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afda26b",
   "metadata": {},
   "source": [
    "**Duplicated and Missing Values:**\n",
    "\n",
    "Our initialy analysis releaved that 199 of these rows were duplicate values, and the following rows contained null / missing values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63cfdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d05ff5",
   "metadata": {},
   "source": [
    "After analyzing the dataset, the following fields were dropped: \n",
    "\n",
    "- Columns with excessive null / missing value counts:\n",
    "  - Samples missing more than 60% of their values were dropped because such rows likely lack sufficient information to contribute meaningfully to the analysis or model training.\n",
    "\n",
    "- Samples with missing target values:\n",
    "  - Any sample missing a target variable value were removed, as they could not meaningfully contribute to a supervised learning task where such a target variable value would be required.\n",
    "\n",
    "- Extreme Outliers in the Target Varaiable:\n",
    "  - We removed samples that are outliers in the target variable (using the IQR method) because these extreme values might be due to data errors or represent atypical cases that could skew the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c24801",
   "metadata": {},
   "source": [
    "**Categorical Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e8f958",
   "metadata": {},
   "source": [
    "Further analysis revealed that the following fields were categorical: \n",
    " \n",
    "- hashottuborspa\n",
    "- propertycountylandusecode\n",
    "- propertyzoningdesc\n",
    "- fireplaceflag\n",
    "- taxdelinquencyflag\n",
    "\n",
    "And additionally, the dataset includes several enumerated fields that, while technically numeric, represent categorical data. While there were many of these fields, the following had variable mappings that were able to be paired from Zillow's Kaggle enumeration key (taken from the confirmed - orgiginal dataset in reference 1).\n",
    "\n",
    "- HeatingOrSystemTypeID\n",
    "- PropertyLandUseTypeID\n",
    "- StoryTypeID\n",
    "- AirConditioningTypeID\n",
    "- ArchitecturalStyleTypeID\n",
    "- TypeConstructionTypeID\n",
    "- BuildingClassTypeID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5026bb67",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d906a-bd9a-42f8-8eb4-7bba1b1b108a",
   "metadata": {},
   "source": [
    "#### 4. Methodology (What you did, and why)  [12 pts]\n",
    "\n",
    "**Focus this section entirely on the steps you took and your reasoning behind them. Emphasize the process and decision-making, not the results themselves**\n",
    "\n",
    "\n",
    "**A. Clearly outline your data cleaning and preprocessing steps**\n",
    "  - Describe what issues you encountered in the raw data and how you addressed them.\n",
    "  - Mention any key decisions (e.g., removing samples with too many missing values).\n",
    "  - What worked and what didn't work?\n",
    "\n",
    "**B. Describe your feature engineering approach**\n",
    "  - Explain any transformations, combinations, or derived features.\n",
    "  - Discuss why certain features were chosen or created, even if they were later discarded.\n",
    "  - What worked and what didn't work?\n",
    "\n",
    "**C. Describe your analytical framework**\n",
    "  - Use of validation curves to see the effect of various hyperparameter choices, and\n",
    "  - Choice of RMSE as primary error metric\n",
    "\n",
    "**D. Detail your model selection process**\n",
    "  - Outline the models you experimented with and why.\n",
    "  - Discuss how you evaluated generalization (e.g., cross-validation, shape and relationships of plots).\n",
    "  - Mention how you tuned hyperparameters or selected the final model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e498366d",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276de7c1-71ab-4bca-b1d4-e78979cbd0b2",
   "metadata": {},
   "source": [
    "**4.A. Data Cleaning and Preprocessing**\n",
    "\n",
    "The first step that we took was to do a preliminary dataset analysis. The main point of this excercise was to get a handle on the different features of the dataset, their structure, if there were a large number of null values, (basically is a feature useable or not) and whether each field had a large degree of outliers. Some quick references to this work are here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404579a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots from part 1? or something interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f187b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaa16ff",
   "metadata": {},
   "source": [
    "From there, we immediately flagged certain features for removal due to having more than 90% null or missing values. However, instead of dropping some variables outright, we agreed as a team to flag specific fields for potential consolidation. For example, 'fireplacecnt' and 'fireplaceflag' both of which were incomplete on their own and displayed instanced where one of them was null and the other was not, could be combined into a single binary flag, such as fireplaceflag_new. Although each of these fields appeared incomplete individually, we believed they could still provide meaningful information contributing to the overall tax value of the home, and therefore be useful for modeling in some capacity.\n",
    "\n",
    "Dropped Features with ≥ 90% Missing Values:\n",
    "\n",
    "- buildingclasstypeid (99.98% missing, numeric)\n",
    "- finishedsquarefeet6 (99.5% missing, numeric)\n",
    "- finishedsquarefeet13 (99.95% missing, numeric)\n",
    "- finishedsquarefeet15 (96.1% missing, numeric)\n",
    "- finishedsquarefeet50 (92.22% missing, numeric)\n",
    "- storytypeid (99.94% missing, numeric)\n",
    "- architecturalstyletypeid (99.73% missing, numeric)\n",
    "- typeconstructiontypeid (99.71% missing, numeric)\n",
    "- decktypeid (99.21% missing, numeric)\n",
    "- taxdelinquencyflag (96.26% missing, categorical)\n",
    "- taxdelinquencyyear (96.26% missing, numeric)\n",
    "- finishedfloor1squarefeet (92.22% missing, numeric)\n",
    "\n",
    "Dropped Features with ≥ 60% Missing Values (Deemed Not Useful for Regression):\n",
    "- threequarterbathnbr (86.98% missing, numeric)\n",
    "- numberofstories (77.32% missing, numeric)\n",
    "- regionidneighborhood (60.09% missing, numeric)\n",
    "\n",
    "We removed features with extremely high percentages of missing data because their lack of completeness compromised their reliability, making them unsuitable for predictive modeling. Columns with ≥90% missing values—such as various square footage measures and identifiers like buildingclasstypeid—lacked sufficient data to offer meaningful insights. Additionally, features with 60% or more missing values, like threequarterbathnbr and numberofstories, were excluded when they provided little value to our regression task. By eliminating these columns, we aimed to reduce noise and potential bias, ensuring that our models are trained on a more robust and complete dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75961c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential display? \n",
    "\n",
    "#  Null threshold: 60.0%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6db226",
   "metadata": {},
   "source": [
    "Next, we looked into records that had null values for the target variable, we figured in this case, that no matter what these records were useless for modeling so they were dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples before dropping null values in the target: 77,613\n",
    "# Number of samples after dropping null values in the target: 77,578\n",
    "# Number of samples before dropping too many null values in the target: 77,578\n",
    "# Number of samples before dropping too many null values in the target: 77,274"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95115dcf",
   "metadata": {},
   "source": [
    "**4.B. Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Feature Engineering Choices from Milestone 2 part 2? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e277966",
   "metadata": {},
   "source": [
    "**4.C. Describe your Analytical Framework**\n",
    "  - Use of validation curves to see the effect of various hyperparameter choices, and\n",
    "  - Choice of RMSE as primary error metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09bc56d",
   "metadata": {},
   "source": [
    "Before proceeding further, we want to clarify that Root Mean Squared Error (RMSE) was used as the primary evaluation metric throughout this analysis. This choice was driven by one key consideration: RMSE is expressed in the same units as the target variable. This makes it an intuitive and meaningful measure of model performance for predicting our target, tax assessed value, in this context -- allowing us to say: 'our predictions are off by 'x' amount on average.'\n",
    "\n",
    "Additionally, given that this model is intended to be a client-facing feature, accuracy becomes even more critical. Clients would likely be dissatisfied if our predictions of their home’s tax assessed value deviate significantly from the actual value. In this context, the difference between predicted and actual values is effectively captured by RMSE. As such, RMSE represents the key source of error we aim to minimize, and it serves as the primary benchmark by which our model’s performance will be evaluated once deployed in production.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cbf573",
   "metadata": {},
   "source": [
    "Going into hyperparameter tuning, our MEAN CV RMSE's were as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c7fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Graph from part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca87b19e",
   "metadata": {},
   "source": [
    "As such our top performing models going into the hyper parameter tuning section of model development were: \n",
    "\n",
    "- Gradient boosted trees with features from Part 2.2 -- no log transformation / RMSE: 402,691\n",
    "- Random Forest Regression with featuers from Part 2.2 -- no log transformation / RMSE: 410,873\n",
    "- Linear Regression with Final Features from Part 3 / RMSE: 411,238"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b109c",
   "metadata": {},
   "source": [
    "For each of these models, the most 'influential' feautres were determined for each using model documentation and Boston Univeristy materials on the issue. From there, parameter sweeps were set up to first test a broad range of possible values for hyperparameters, and results were plotted for the mean CV for each of the hyperparameters. \n",
    "\n",
    "From there, we used the plots in the first parameter sweeps to either: a) determine a second parameter sweep (if necessary) using curtailed ranges in order to zero in on the best possible range, or b) determine a final range to test for each hyperparameter using Gridsearch Cross Validation. The results are as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07cf836",
   "metadata": {},
   "source": [
    "**Gradient Boosted Trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Trees Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9e027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Gridsearch and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc0c01",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8144296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First parameter sweep results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6113b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated ranges if needed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d028c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch CV and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f514022",
   "metadata": {},
   "source": [
    "**Linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f401d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First parameter sweep results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c61cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated ranges if needed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a750e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch CV and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a42884f",
   "metadata": {},
   "source": [
    "**4.D. Detail your model selection process**\n",
    "  - Outline the models you experimented with and why.\n",
    "  - Discuss how you evaluated generalization (e.g., cross-validation, shape and relationships of plots).\n",
    "  - Mention how you tuned hyperparameters or selected the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a3855",
   "metadata": {},
   "source": [
    "At the beginning of Part 2, we developed a function called run_model(), which was used throughout the project to consistently evaluate model performance across different stages. The output of run_model() included:\n",
    "\n",
    "- An updated global output DataFrame that recorded results each time the function was executed, allowing us to track progress across multiple tests.\n",
    "- An optional plot comparing actual vs. predicted values for each model, enhanced with a heatmap to visualize how closely points aligned with the ideal prediction line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb18981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e432a3",
   "metadata": {},
   "source": [
    "These visualizations helped us assess model behavior, particularly in identifying signs of overfitting and understanding performances across different ranges of the target variable. For example, some models predicted lower-value targets accurately but struggled with higher-value predictions, especially when the target variable exceeded $2 million.\n",
    "\n",
    "Additionally, run_model() tracked Root Mean Squared Error (RMSE) at three key stages:\n",
    "- Training RMSE\n",
    "- Cross-Validation RMSE using 5-fold, 5-repeat cross-validation\n",
    "- Test Set RMSE (utlilized at the end of all tests)\n",
    "\n",
    "As an a side note, for cases where the target variable was log-transformed, we implemented a parameter within run_model() to handle this scenario. The function applied cross-validation in log space and then inverse-transformed the predictions to calculate RMSE in the original scale. This approach ensured consistency and leveraged KFold to properly simulate repeated cross validation. \n",
    "\n",
    "At the conclusion of each phase — whether baseline testing, feature engineering, or feature selection — we plotted the Training RMSE against the Cross-Validation RMSE for all candidate models. This allowed us to visually compare performance and detect potential overfitting by observing discrepancies between training and validation errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47237ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of RMSE plot for baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe63e0c",
   "metadata": {},
   "source": [
    "Finally, after completing feature selection, we identified the top-performing models for hyperparameter tuning based on their performance and generalization capability. The selected models were:\n",
    "\n",
    "- Gradient Boosted Trees with full features and no log transformation\n",
    "- Random Forest with full features and no log transformation\n",
    "- Linear Regression with selected features and no log transformation\n",
    "\n",
    "These models demonstrated the best balance between accuracy and robustness, making them strong candidates for further optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c94d034",
   "metadata": {},
   "source": [
    "Starting with each of the top three models, the four 'most influencial' hyperparamters were chosen based on documentation and Boston University resources. **(Need reference)**. For the **gradient boosted Trees** model these were: \n",
    "\n",
    "- **learning_rate** - (impact of each tree on learning  (typicall 0 - 1); lower values indicate slower learning)\n",
    "- **n_estimators** - (Essentially how many trees (in this case boosting stages) are used)\n",
    "- **max_depth** - (Limits how deep each tree can grow)\n",
    "- **max_features** - (How many features are considered at each split)\n",
    "\n",
    "For the **Random Forest** model they were: \n",
    "- **n_estimators** - (Essentially, how many trees are used)\n",
    "- **max_features** - (How many feautures are considered for each split)\n",
    "- **max_depth** - (How deep each tree can grow)\n",
    "- **booststrap** - (Is bootstrapping used (T/F))\n",
    "\n",
    "Finally, the **Linear Regression** model didn’t offer many hyperparameters to tune, but the two we could adjust were:\n",
    "- **fit_intercepts** (Force the model to go through the origin if False (T/F))\n",
    "- **positive** (Force the model to only have positive coefficients if True (T/F))\n",
    "\n",
    "\n",
    "The details of our specific hyperparameter choices are outlined in Part 4.C. In general, we used the full-feature, no-log dataset for training, as our top-performing models consistently achieved the best results on this dataset, and we aimed to maintain standardization across tests.\n",
    "\n",
    "We began by defining broad initial ranges for each hyperparameter to observe their impact on RMSE cross-validation scores. Based on these results, we refined our search by conducting a second parameter sweep to “hone in” on ranges that showed promising behavior. Finally, we used these insights to define a focused range of values for a GridSearchCV to fine-tune the models.\n",
    "\n",
    "Once we finalized the hyperparameter selections for each model, we conducted final tests using three datasets:\n",
    "\t1.\tThe no-log full-feature dataset,\n",
    "\t2.\tThe logged-target dataset, and\n",
    "\t3.\tThe dataset with selected features from feature selection.\n",
    "\n",
    "We compared the performance of these tuned models against all prior runs—from baseline models to those with feature engineering and so on — to identify the overall best-performing model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a3ad0",
   "metadata": {},
   "source": [
    "__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c543064",
   "metadata": {},
   "source": [
    "**Graveyard**\n",
    "For each of these, an initial parameter sweep was set up to test wide ranges for each of the initial parameters. Our initial ranges were as follows: \n",
    "\n",
    "- **n_estimators:** -- Set up an initial range of (100 - 500 estimators). The default number of estimators for both the random forest and the gradient boosted trees models were 100 estimators, so this is to test if there is a significant impact to increasing the number of estimators to a large degree. IF this did not prove true (or if the curve was just flat) then a second range from 1 - 100 was proposed to see if there was a significant impact on *decreasing* the overall number of estimators. \n",
    "\n",
    "- **learning_rate** -- This hyperparameter was only on the boosted tree set. We initially set up an initial test of 0.05, 0.1, 0.25, 0.5, 0.75, 1.0 and realized that we got a relatively flat performance curve, with a 'best' value on the lower end of the curve. A second sweep was set up to test the lower end of the range with 0.01, 0.05, 0.1, 0.2, 0.3 being the possible values. At this point we got a 'best' value of around 0.1, so gridsearch CV was setup to test 0.09, 0.1, 0.11, 0.12, and 0.13 for the final test. \n",
    "\n",
    "- **max_depth** - An initial run of 1 - the max features was set up to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52326d",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305c55a-370f-4083-8cb6-395545ff1013",
   "metadata": {},
   "source": [
    "#### 5. Results and Evaluation (What you found, and how well it worked) [10 pts]\n",
    "\n",
    "**Focus purely on outcomes, with metrics, visuals, and insights. This is where you present evidence to support your conclusions.**\n",
    "\n",
    "- Provide a clear and detailed narrative of your analysis and reasoning using the analytical approach described in (4). \n",
    "- Discuss model performance metrics and results (RMSE, R2, etc.)\n",
    "- **Include relevant visualizations (graphs, charts, tables) with appropriate labels and captions**\n",
    "- Error analysis\n",
    "  - Highlight specific patterns of error, outliers, or questionable features.\n",
    "  - Note anything surprising or worth improving in future iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7615213f-7689-47fe-ac5a-24a767faaae5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04890c67",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97153d2-e099-4c15-99f8-ad0b6a539d4b",
   "metadata": {},
   "source": [
    "#### 6. Conclusion [2 pts]\n",
    "- Clearly state your main findings and how they address your original objectives\n",
    "- Highlight the business or practical implications of your findings \n",
    "- Discuss the limitations and constraints of your analysis clearly and transparently\n",
    "- Suggest potential improvements or future directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8287f955-7ae0-41ec-864d-44765c60ffe7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1dc34530",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc0f7d",
   "metadata": {},
   "source": [
    "## **References**  \n",
    "1. Kaggle. (2017). Zillow Prize: Zillow’s Home Value Prediction (Zestimate) [Dataset]. Kaggle. https://www.kaggle.com/competitions/zillow-prize-1/data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899325dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
